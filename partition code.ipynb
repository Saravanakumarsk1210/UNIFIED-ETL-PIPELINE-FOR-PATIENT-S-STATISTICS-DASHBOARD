{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692faa00-bf1e-4b78-8b03-8d9fbbf7e767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade8d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, schema_of_json\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSparkPartitionConsumer\") \\\n",
    "    .config(\"spark.jars\", \"/usr/share/java/postgresql-42.7.4.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# PostgreSQL connection parameters\n",
    "jdbc_url_target = \"jdbc:postgresql://host.docker.internal:5432/postgres\" # your database name in postgres\n",
    "jdbc_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"2003\", # your password\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Read a single Kafka message to infer schema\n",
    "sample_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"postgres.public.patients1\") \\ \n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \\\n",
    "    .limit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce4f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract a sample JSON string and infer schema\n",
    "sample_json = sample_df.selectExpr(\"CAST(value AS STRING) as json_string\").first()[\"json_string\"]\n",
    "inferred_schema = schema_of_json(sample_json)\n",
    "\n",
    "# Read data from Kafka with inferred schema\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"postgres.public.patients1\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert Kafka JSON messages to structured data dynamically\n",
    "df = df.selectExpr(\"CAST(value AS STRING) as json_string\") \\\n",
    "    .select(from_json(col(\"json_string\"), inferred_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")  # Extract fields dynamically\n",
    "\n",
    "# Define partition mapping\n",
    "partition_name_map = {\n",
    "    0: \"Tirunelveli\",\n",
    "    1: \"Madurai\",\n",
    "    2: \"Salem\",\n",
    "    3: \"Coimbatore\",\n",
    "    4: \"Tiruchirappalli\",\n",
    "    5: \"Chennai\"\n",
    "}\n",
    "\n",
    "# Function to write data to PostgreSQL\n",
    "def write_to_postgres(batch_df, batch_id):\n",
    "    if batch_df.count() == 0:\n",
    "        return  # Skip empty batches\n",
    "    \n",
    "    # Identify partition dynamically\n",
    "    unique_cities = batch_df.select(\"cityid\").distinct().collect()\n",
    "    \n",
    "    for row in unique_cities:\n",
    "        city_id = row[\"cityid\"]\n",
    "        if city_id in partition_name_map:\n",
    "            table_name = f\"patients_{partition_name_map[city_id]}\"\n",
    "            partition_df = batch_df.filter(col(\"cityid\") == city_id)\n",
    "            partition_df.write.jdbc(url=jdbc_url_target, table=table_name, mode=\"append\", properties=jdbc_properties)\n",
    "\n",
    "# Start streaming\n",
    "query = df.writeStream \\\n",
    "    .foreachBatch(write_to_postgres) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
